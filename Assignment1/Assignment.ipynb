{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, X, y, cv=5,verbose=False):\n",
    "    test_error = []\n",
    "    for i, (train_index, test_index) in enumerate(KFold(n_splits=cv).split(X)):\n",
    "        model.fit(X[train_index], y[train_index])\n",
    "        y_pred = model.predict(X[test_index])\n",
    "        test_error.append(root_mean_squared_error(y[test_index], y_pred))\n",
    "        if verbose:\n",
    "            print(f\"Fold {i+1}: {root_mean_squared_error(y[test_index], y_pred)}\")\n",
    "    print(f\"Model: {model.__class__.__name__}\")\n",
    "    print(f\"Mean: {np.mean(test_error)}\")\n",
    "    print(f\"Std: {np.std(test_error)}\")\n",
    "    return test_error\n",
    "def centerData(data):\n",
    "    mu = np.mean(data,axis=0)\n",
    "    data = data - mu\n",
    "    return data, mu\n",
    "def normalize(X):\n",
    "    d = np.linalg.norm(X,axis=0,ord=2)\n",
    "    d[d==0]=1\n",
    "    X_pre = X / d\n",
    "    return X_pre,d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('case1Data.txt')\n",
    "raw_data = raw_data.replace([' NaN','NaN',' NaN ','NaN '], np.nan)\n",
    "y = raw_data['y']\n",
    "X_num = raw_data.loc[:, ' x_ 1':' x_95'].astype(float) \n",
    "X_cat = raw_data.loc[:, ' C_ 1':' C_ 5']\n",
    "X_cat_np = X_cat.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cat_features = []\n",
    "for i in range(X_cat.shape[1]):\n",
    "    unique_values = X_cat.iloc[:, i].unique()\n",
    "    unique_values = unique_values[~pd.isna(unique_values)]  # Filter out nan values\n",
    "    unique_values = np.append(unique_values, np.nan)\n",
    "    X_cat_features.append(unique_values)\n",
    "encoder = OneHotEncoder(categories=X_cat_features)\n",
    "X_cat_encoded = encoder.fit_transform(X_cat_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {'G': 1, 'H': 2, 'I': 3, 'J': 4, 'K': 5, 'Na': 6}\n",
    "X_cat_label = (X_cat.fillna('Na')).applymap(lambda x: mapping.get(str(x).strip(), x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = SimpleImputer(strategy='mean')\n",
    "X = pd.concat([X_num, X_cat_label], axis=1)\n",
    "X_imp = imp.fit_transform(X)\n",
    "X_imp_standardized = normalize(X_imp)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test models: Right branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 230,\n",
       " 'loss': 'linear',\n",
       " 'learning_rate': 0.3736734693877551,\n",
       " 'estimator': LassoCV(max_iter=50000)}"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_estimators = [int(x) for x in np.linspace(start=10, stop=1000, num=10)]\n",
    "learning_rate = np.linspace(0.01, 1, 50)\n",
    "loss = ['linear']\n",
    "estimator = [LassoCV(max_iter=50000), RidgeCV()]\n",
    "\n",
    "param_grid = {'estimator' : estimator,\n",
    "                'n_estimators': n_estimators,\n",
    "                'learning_rate': learning_rate,\n",
    "                'loss': loss}\n",
    "\n",
    "ada = AdaBoostRegressor()\n",
    "ada_random = RandomizedSearchCV(estimator=ada, param_distributions=param_grid, n_iter=50, cv=3, verbose=2, n_jobs=-1)\n",
    "ada_random.fit(X_imp_standardized, y)\n",
    "ada_random.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: ElasticNet\n",
      "Mean: 33.06005422006701\n",
      "Std: 9.346825664936446\n",
      "Model: Lasso\n",
      "Mean: 28.21841102874767\n",
      "Std: 6.6131964893711945\n",
      "Model: RandomForestRegressor\n",
      "Mean: 40.028253896420594\n",
      "Std: 9.248198292423398\n",
      "Model: AdaBoostRegressor\n",
      "Mean: 26.68836014507527\n",
      "Std: 4.129760396377916\n",
      "Model: HistGradientBoostingRegressor\n",
      "Mean: 46.73235969430247\n",
      "Std: 8.091318808784829\n"
     ]
    }
   ],
   "source": [
    "# Elastic net\n",
    "RMSE_elastic = test_model(ElasticNet(alpha=0.3359818286283781, l1_ratio=1.0), X_imp_standardized, y, cv=5, verbose=False)\n",
    "# Lasso\n",
    "RMSE_lasso = test_model(Lasso(alpha=0.14), X_imp_standardized, y, cv=5, verbose=False)\n",
    "# Random forest\n",
    "RMSE_rf = test_model(RandomForestRegressor(n_estimators=600, min_samples_split=5, min_samples_leaf=4, max_depth=80, bootstrap=True), X_imp_standardized, y, cv=5, verbose=False)\n",
    "# AdaBoost\n",
    "RMSE_ada = test_model(AdaBoostRegressor(n_estimators=230, loss='linear', learning_rate=0.373673469387755, estimator=LassoCV(max_iter=50000, tol=0.001)), X_imp_standardized, y, cv=5, verbose=False)\n",
    "# HistGradientBoosting\n",
    "RMSE_gradient = test_model(HistGradientBoostingRegressor(validation_fraction=0.2, random_state=0, min_samples_leaf=4, max_leaf_nodes=None, max_iter=100, max_depth=20, learning_rate=0.01, early_stopping=True), X_imp_standardized, y, cv=5, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test models: Left Branch\n",
    "### Model Encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: AdaBoostRegressor\n",
      "Mean: 59.435631780904444\n",
      "Std: 5.258857081677979\n",
      "Model: AdaBoostRegressor\n",
      "Mean: 56.093416617688625\n",
      "Std: 3.6476401124448574\n",
      "Model: RandomForestRegressor\n",
      "Mean: 57.8702772804999\n",
      "Std: 7.311605823508181\n",
      "Model: RandomForestRegressor\n",
      "Mean: 57.27495010766796\n",
      "Std: 6.919773969832535\n",
      "Model: Lasso\n",
      "Mean: 55.337393065050364\n",
      "Std: 5.485788484586642\n",
      "Model: Lasso\n",
      "Mean: 55.25918395265819\n",
      "Std: 5.071644891941095\n",
      "Model: Ridge\n",
      "Mean: 54.95671896993749\n",
      "Std: 5.2926474975676925\n",
      "Model: Ridge\n",
      "Mean: 55.01036751814229\n",
      "Std: 5.2807394832245365\n"
     ]
    }
   ],
   "source": [
    "# AdaBoost OneHot\n",
    "RMSE_ada_OneHot = test_model(AdaBoostRegressor(n_estimators=2000, loss='linear', learning_rate=0.5757142857142857, estimator=LassoCV(max_iter=5000)), X_cat_encoded, y, cv=5, verbose=False)\n",
    "# AdaBoost Label\n",
    "RMSE_ada_Label = test_model(AdaBoostRegressor(n_estimators=2000, loss='linear', learning_rate=0.676734693877551, estimator=LassoCV(max_iter=5000)), X_cat_label.to_numpy(), y, cv=5, verbose=False)\n",
    "# Random forest OneHot\n",
    "RMSE_rf_OneHot = test_model(RandomForestRegressor(n_estimators=400, min_samples_split=10, min_samples_leaf=4, max_depth=60, bootstrap=True), X_cat_encoded, y, cv=5, verbose=False)\n",
    "RMSE_rf_Label = test_model(RandomForestRegressor(n_estimators=400, min_samples_split=10, min_samples_leaf=4, max_depth=60, bootstrap=True), X_cat_label.to_numpy(), y, cv=5, verbose=False)\n",
    "# Lasso OneHot\n",
    "RMSE_lasso_OneHot = test_model(Lasso(alpha=4.842528290199999), X_cat_encoded, y, cv=5, verbose=False)\n",
    "# Lasso Label\n",
    "RMSE_lasso_Label = test_model(Lasso(alpha=9.6610262932), X_cat_label.to_numpy(), y, cv=5, verbose=False)\n",
    "# Ridge OneHot\n",
    "RMSE_ridge_OneHot = test_model(Ridge(alpha=170.26846846846846), X_cat_encoded, y, cv=5, verbose=False)\n",
    "# Ridge Label\n",
    "RMSE_ridge_label = test_model(Ridge(alpha=10000), X_cat_label.to_numpy(), y, cv=5, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = Ridge(alpha=170.26846846846846)\n",
    "r.fit(X_cat_encoded, y)\n",
    "y_pred_cat = r.predict(X_cat_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_left = X_num\n",
    "X_left['y_cat'] = y_pred_cat\n",
    "X_left_imp = imp.fit_transform(X_left)\n",
    "X_left_imp_standardized = normalize(X_left_imp)[0]\n",
    "X = X_left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal alpha: 0.13721212121212112\n",
      "Associated mean test RMSE: 24.57477971581286\n",
      "Standard error of the mean test RMSE: 2.8740824808931182\n",
      "Associated mean training RMSE for optimal alpha: 14.611151617695999\n"
     ]
    }
   ],
   "source": [
    "alphas = np.linspace(0.9, 0.001, 100)  # Range of alphas to test\n",
    "CV = 5  # Number of cross-validation folds\n",
    "kf = KFold(n_splits=CV)\n",
    "\n",
    "# Prepare arrays to store results\n",
    "Err_tr = np.zeros((CV, len(alphas)))\n",
    "Err_tst = np.zeros((CV, len(alphas)))\n",
    "\n",
    "# Start cross-validation\n",
    "for i, (train_index, test_index) in enumerate(kf.split(X,y)):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # Data preprocessing (impute, center, normalize)\n",
    "    imp = SimpleImputer(strategy='mean')\n",
    "    X_train_imputed = imp.fit_transform(X_train)\n",
    "    X_test_imputed = imp.transform(X_test)\n",
    "\n",
    "    # Assuming centerData and normalize are functions you've defined previously\n",
    "    y_train, y_mean = centerData(y_train)  # Center training response\n",
    "    y_test = y_test - y_mean                # Center test response using the same mean\n",
    "\n",
    "    X_train_imputed, x_mu = centerData(X_train_imputed)  # Center training data\n",
    "    X_test_imputed = X_test_imputed - x_mu               # Center test data using the same mean\n",
    "\n",
    "    X_train_imputed, x_scale = normalize(X_train_imputed)  # Normalize training data\n",
    "    X_test_imputed = X_test_imputed / x_scale              # Normalize test data using the same scale\n",
    "\n",
    "    # Iterate through alphas\n",
    "    for j, alpha in enumerate(alphas):\n",
    "        # Fit Lasso model with current alpha\n",
    "        reg = Lasso(alpha=alpha,max_iter=10000)\n",
    "        reg.fit(X_train_imputed, y_train)\n",
    "        \n",
    "        # Predict and find error for both train and test datasets\n",
    "        YhatTr = reg.predict(X_train_imputed)  # Use the model's predict method\n",
    "        YhatTest = reg.predict(X_test_imputed)  # Use the model's predict method\n",
    "        \n",
    "        # Store the training and test errors (MSE)\n",
    "        Err_tr[i, j] = root_mean_squared_error(y_train, YhatTr)  # Training error\n",
    "        Err_tst[i, j] = root_mean_squared_error(y_test, YhatTest) # Test error\n",
    "\n",
    "# Calculate the average RMSE over all CV folds for each alpha\n",
    "mean_err_tr = np.mean(Err_tr, axis=0)  # Average training RMSE for each alpha\n",
    "mean_err_tst = np.mean(Err_tst, axis=0)  # Average test RMSE for each alpha\n",
    "std_err_tst = np.std(Err_tst, axis=0) / np.sqrt(CV)  # Standard error of test RMSE for each alpha\n",
    "\n",
    "# Find the index of the smallest average test RMSE\n",
    "optimal_alpha_index = np.argmin(mean_err_tst)\n",
    "optimal_alpha = alphas[optimal_alpha_index]\n",
    "optimal_err_tst = mean_err_tst[optimal_alpha_index]\n",
    "optimal_err_ste = std_err_tst[optimal_alpha_index]\n",
    "\n",
    "# Print the details of the optimal model\n",
    "print(\"Optimal alpha:\", optimal_alpha)\n",
    "print(\"Associated mean test RMSE:\", optimal_err_tst)\n",
    "print(\"Standard error of the mean test RMSE:\", optimal_err_ste)\n",
    "print(\"Associated mean training RMSE for optimal alpha:\", mean_err_tr[optimal_alpha_index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Lasso\n",
      "Mean: 28.14889796451639\n",
      "Std: 6.5607369904197075\n",
      "Model: RandomForestRegressor\n",
      "Mean: 39.730432675572104\n",
      "Std: 9.322859848372044\n",
      "Model: AdaBoostRegressor\n",
      "Mean: 25.554154540204035\n",
      "Std: 4.895661596760893\n"
     ]
    }
   ],
   "source": [
    "# Lasso Left\n",
    "RMSE_Left_lasso = test_model(Lasso(alpha=0.13721212121212112), X_left_imp_standardized, y, cv=5, verbose=False)\n",
    "# Random forest Left\n",
    "RMSE_left_rf = test_model(RandomForestRegressor(n_estimators=800, min_samples_split=2, min_samples_leaf=1, max_depth=110, bootstrap=True), X_left_imp_standardized, y, cv=5, verbose=False)\n",
    "# adaBoost Left\n",
    "RMSE_ada_Lasso = test_model(AdaBoostRegressor(n_estimators=300, loss='linear', learning_rate=0.30204081632653063, estimator=LassoCV(max_iter=5000,tol=0.01)), X_left_imp_standardized, y, cv=5, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
